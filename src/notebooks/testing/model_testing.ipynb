{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Model testing\n",
    "Trying to identify how models work in different configurations"
   ],
   "id": "cfc0820b8d0a5f2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T20:59:43.706583Z",
     "start_time": "2025-05-06T20:59:32.224471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from src.myscripts import prepare_data\n",
    "import pandas as pd\n",
    "import contextlib\n",
    "import sys\n",
    "from sklearn.metrics import accuracy_score\n",
    "from src.myscripts.model import Conv1DClassifier\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import config\n",
    "from src.myscripts.train import ModelTrainer\n",
    "@contextlib.contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, 'w') as fnull:\n",
    "        old_stdout = sys.stdout\n",
    "        try:\n",
    "            sys.stdout = fnull\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_avarage_model_accuracy(model, device, data, num_epochs:int, early_stopping_rounds:int, num_of_samples:int):\n",
    "    acc_sum=0\n",
    "    \n",
    "    for i in range(num_of_samples):\n",
    "        model_trainer = ModelTrainer(copy.deepcopy(model), device)\n",
    "        x_train, y_train, x_val, y_val, x_test, y_test = prepare_data.prepare_data_for_model(data, -1, LabelEncoder, [0.7, 0.2, 0.1], random_state=np.random.randint(0,10000), save_to_npy=False)\n",
    "        x_train = x_train.astype(np.float32)\n",
    "        x_val = x_val.astype(np.float32)\n",
    "        y_train = y_train.astype(\"long\")\n",
    "        y_val = y_val.astype(\"long\")\n",
    "        x_test = x_test.astype(np.float32)\n",
    "        y_test = y_test.astype(\"long\")\n",
    "        x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "        y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "        train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "        val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "        x_test_tensor = torch.tensor(x_test).to(device)\n",
    "        if len(y_test.shape) > 1 and y_test.shape[1] > 1:\n",
    "            true_labels = np.argmax(y_test, axis=1)\n",
    "        else:\n",
    "            true_labels = y_test\n",
    "    \n",
    "        with suppress_stdout():\n",
    "            model_trainer.train_model(train_dataset, val_dataset, epochs=num_epochs,early_stopping_rounds=early_stopping_rounds)\n",
    "        sample_model=model_trainer.get_trained_model()\n",
    "        sample_model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = sample_model(x_test_tensor)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "        \n",
    "        predictions = predictions.cpu().numpy()\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        acc_sum+=accuracy\n",
    "        print(f\"\\rSamples done: {i+1}/{num_of_samples}\\033[K\", end=\"\")\n",
    "    print(\"\\n--------------\")\n",
    "    print(f\"Avarage accuracy of model in {num_of_samples} samples is: {acc_sum/num_of_samples}\")\n",
    "    return acc_sum/num_of_samples\n",
    "    "
   ],
   "id": "772d44860055bfe3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bcom_\\Documents\\Projekty\\Rozpoznawanie_dzwiekow_gitarowych\\data\\prepared_data\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Testing different dataset combinations",
   "id": "8772b54b6a361dfa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T20:59:43.735704Z",
     "start_time": "2025-05-06T20:59:43.719966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.myscripts import func\n",
    "\n",
    "n_fft = 2048 # Ile próbek bierze do okna na ktorym dokonuje transformaty\n",
    "hop_lenght = 1024 # O ile próbek przesuwa okno po każdej transformacie (Od tego zalezy wielkosc dataframe'a)\n",
    "sr = 22050 # Liczba próbek na sekunde (Od tego zalezy wielkosc dataframe'a)"
   ],
   "id": "469939c17f2f4d40",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T20:59:46.133028Z",
     "start_time": "2025-05-06T20:59:44.327988Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
   "id": "c50601c1ccc6d77a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T20:59:46.170284Z",
     "start_time": "2025-05-06T20:59:46.153284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_feature_and_label_combination(model_class, device, early_stopping_rounds=3, num_epochs=15, num_of_samples=3):\n",
    "    labels = ['sound', 'string', 'pluck', 'sound_type']\n",
    "    features= ['mel', 'chroma', 'contrast']\n",
    "    label_combinations = []\n",
    "    feature_combinations = []\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(i + 1, len(labels) + 1):\n",
    "            label_combinations.append(labels[i:j])\n",
    "    label_combinations.sort(key=len)\n",
    "    for i in range(len(features)):\n",
    "        for j in range(i + 1, len(features) + 1):\n",
    "            feature_combinations.append(features[i:j])\n",
    "    feature_combinations.sort(key=len)\n",
    "    test_results_dataframe = pd.DataFrame(columns=['label_combination', 'feature_combination', 'avarage_model_accuracy'])\n",
    "    for l in label_combinations:\n",
    "        for f in feature_combinations:\n",
    "            data = func.get_feature_combination_dataframe(f,l,n_fft,hop_lenght,sr,config.SOUNDS_DATA_DIR_PATH)\n",
    "            x_train, _, _, _, _, _ = prepare_data.prepare_data_for_model(data, -1, LabelEncoder, [0.7, 0.2, 0.1], save_to_npy=False)\n",
    "            input_shape = x_train.shape[1:]\n",
    "            num_of_classes = data[\"label\"].nunique()\n",
    "            model = model_class(num_classes=num_of_classes, input_shape=input_shape)\n",
    "            avg_val = evaluate_avarage_model_accuracy(model, device, data, early_stopping_rounds=early_stopping_rounds, num_epochs=num_epochs, num_of_samples=num_of_samples)\n",
    "            test_results_dataframe.loc[len(test_results_dataframe)]= [l,f,avg_val]\n",
    "            print(f\"Finished {l}+{f}\")\n",
    "    return test_results_dataframe"
   ],
   "id": "3b0776522475cedc",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T21:49:53.155471Z",
     "start_time": "2025-05-06T20:59:46.188956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = test_feature_and_label_combination(Conv1DClassifier, device, num_of_samples=3)\n",
    "result"
   ],
   "id": "b4d71eb10061f899",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.9931506849315067\n",
      "Finished ['sound']+['mel']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.8447488584474886\n",
      "Finished ['sound']+['chroma']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.1735159817351598\n",
      "Finished ['sound']+['contrast']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.9863013698630136\n",
      "Finished ['sound']+['mel', 'chroma']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.7191780821917808\n",
      "Finished ['sound']+['chroma', 'contrast']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.9931506849315067\n",
      "Finished ['sound']+['mel', 'chroma', 'contrast']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.9794520547945206\n",
      "Finished ['string']+['mel']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.9086757990867579\n",
      "Finished ['string']+['chroma']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.9566210045662101\n",
      "Finished ['string']+['contrast']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.9703196347031963\n",
      "Finished ['string']+['mel', 'chroma']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.952054794520548\n",
      "Finished ['string']+['chroma', 'contrast']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.9817351598173515\n",
      "Finished ['string']+['mel', 'chroma', 'contrast']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.4931506849315068\n",
      "Finished ['pluck']+['mel']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.4794520547945205\n",
      "Finished ['pluck']+['chroma']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.5639269406392694\n",
      "Finished ['pluck']+['contrast']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.5205479452054794\n",
      "Finished ['pluck']+['mel', 'chroma']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.5502283105022832\n",
      "Finished ['pluck']+['chroma', 'contrast']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.502283105022831\n",
      "Finished ['pluck']+['mel', 'chroma', 'contrast']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.7602739726027398\n",
      "Finished ['sound_type']+['mel']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.7968036529680366\n",
      "Finished ['sound_type']+['chroma']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.8150684931506849\n",
      "Finished ['sound_type']+['contrast']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.8036529680365296\n",
      "Finished ['sound_type']+['mel', 'chroma']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.8287671232876712\n",
      "Finished ['sound_type']+['chroma', 'contrast']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.8242009132420091\n",
      "Finished ['sound_type']+['mel', 'chroma', 'contrast']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.958904109589041\n",
      "Finished ['sound', 'string']+['mel']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.5639269406392694\n",
      "Finished ['sound', 'string']+['chroma']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.08904109589041094\n",
      "Finished ['sound', 'string']+['contrast']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.9634703196347032\n",
      "Finished ['sound', 'string']+['mel', 'chroma']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.46575342465753433\n",
      "Finished ['sound', 'string']+['chroma', 'contrast']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.952054794520548\n",
      "Finished ['sound', 'string']+['mel', 'chroma', 'contrast']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.589041095890411\n",
      "Finished ['string', 'pluck']+['mel']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.41780821917808214\n",
      "Finished ['string', 'pluck']+['chroma']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.5319634703196348\n",
      "Finished ['string', 'pluck']+['contrast']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.6050228310502282\n",
      "Finished ['string', 'pluck']+['mel', 'chroma']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.5091324200913242\n",
      "Finished ['string', 'pluck']+['chroma', 'contrast']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.5388127853881278\n",
      "Finished ['string', 'pluck']+['mel', 'chroma', 'contrast']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.1643835616438356\n",
      "Finished ['pluck', 'sound_type']+['mel']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.3515981735159817\n",
      "Finished ['pluck', 'sound_type']+['chroma']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.3835616438356164\n",
      "Finished ['pluck', 'sound_type']+['contrast']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.226027397260274\n",
      "Finished ['pluck', 'sound_type']+['mel', 'chroma']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.3675799086757991\n",
      "Finished ['pluck', 'sound_type']+['chroma', 'contrast']\n",
      "Samples done: 3/3\u001B[K\n",
      "--------------\n",
      "Avarage accuracy of model in 3 samples is: 0.2579908675799087\n",
      "Finished ['pluck', 'sound_type']+['mel', 'chroma', 'contrast']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mtest_feature_and_label_combination\u001B[49m\u001B[43m(\u001B[49m\u001B[43mConv1DClassifier\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_of_samples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m result\n",
      "Cell \u001B[1;32mIn[4], line 18\u001B[0m, in \u001B[0;36mtest_feature_and_label_combination\u001B[1;34m(model_class, device, early_stopping_rounds, num_epochs, num_of_samples)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m feature_combinations:\n\u001B[0;32m     17\u001B[0m     data \u001B[38;5;241m=\u001B[39m func\u001B[38;5;241m.\u001B[39mget_feature_combination_dataframe(f,l,n_fft,hop_lenght,sr,config\u001B[38;5;241m.\u001B[39mSOUNDS_DATA_DIR_PATH)\n\u001B[1;32m---> 18\u001B[0m     x_train, _, _, _, _, _ \u001B[38;5;241m=\u001B[39m \u001B[43mprepare_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprepare_data_for_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mLabelEncoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0.7\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave_to_npy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m     input_shape \u001B[38;5;241m=\u001B[39m x_train\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m:]\n\u001B[0;32m     20\u001B[0m     num_of_classes \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mnunique()\n",
      "File \u001B[1;32m~\\Documents\\Projekty\\Rozpoznawanie_dzwiekow_gitarowych\\src\\myscripts\\prepare_data.py:37\u001B[0m, in \u001B[0;36mprepare_data_for_model\u001B[1;34m(df, label_col_index, encoder_class, split_proportions, random_state, save_to_npy)\u001B[0m\n\u001B[0;32m     34\u001B[0m Y \u001B[38;5;241m=\u001B[39m data_encoded\u001B[38;5;241m.\u001B[39miloc[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\n\u001B[0;32m     36\u001B[0m \u001B[38;5;66;03m# Podzial zbioru na dane do uczenia, zbior validacyjny oraz zbior do pozniejszego testowania danych.\u001B[39;00m\n\u001B[1;32m---> 37\u001B[0m x_train, x_test, y_train, y_test \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_test_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43msplit_proportions\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstratify\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mY\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     38\u001B[0m x_val, x_test, y_val, y_test \u001B[38;5;241m=\u001B[39m train_test_split(x_test, y_test, test_size\u001B[38;5;241m=\u001B[39msplit_proportions[\u001B[38;5;241m2\u001B[39m]\u001B[38;5;241m/\u001B[39m(split_proportions[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m+\u001B[39msplit_proportions[\u001B[38;5;241m2\u001B[39m]), random_state\u001B[38;5;241m=\u001B[39mrandom_state, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     39\u001B[0m                                                 stratify\u001B[38;5;241m=\u001B[39my_test)\n\u001B[0;32m     41\u001B[0m \u001B[38;5;66;03m# Rozszerzenie wymiarów danych, ponieważ dla CNN potrzeba 3 wymiarow (wysokosc, szerokosc i ilosc kanalow obrazu)\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\rozpoznawanie_dzwiekow_gitarowy\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    211\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    212\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    213\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    214\u001B[0m         )\n\u001B[0;32m    215\u001B[0m     ):\n\u001B[1;32m--> 216\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    220\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    221\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    222\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    223\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    224\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    225\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    226\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\rozpoznawanie_dzwiekow_gitarowy\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2872\u001B[0m, in \u001B[0;36mtrain_test_split\u001B[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[0m\n\u001B[0;32m   2868\u001B[0m         CVClass \u001B[38;5;241m=\u001B[39m ShuffleSplit\n\u001B[0;32m   2870\u001B[0m     cv \u001B[38;5;241m=\u001B[39m CVClass(test_size\u001B[38;5;241m=\u001B[39mn_test, train_size\u001B[38;5;241m=\u001B[39mn_train, random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[1;32m-> 2872\u001B[0m     train, test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43marrays\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstratify\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2874\u001B[0m train, test \u001B[38;5;241m=\u001B[39m ensure_common_namespace_device(arrays[\u001B[38;5;241m0\u001B[39m], train, test)\n\u001B[0;32m   2876\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\n\u001B[0;32m   2877\u001B[0m     chain\u001B[38;5;241m.\u001B[39mfrom_iterable(\n\u001B[0;32m   2878\u001B[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m arrays\n\u001B[0;32m   2879\u001B[0m     )\n\u001B[0;32m   2880\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\rozpoznawanie_dzwiekow_gitarowy\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:1909\u001B[0m, in \u001B[0;36mBaseShuffleSplit.split\u001B[1;34m(self, X, y, groups)\u001B[0m\n\u001B[0;32m   1879\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001B[39;00m\n\u001B[0;32m   1880\u001B[0m \n\u001B[0;32m   1881\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1906\u001B[0m \u001B[38;5;124;03mto an integer.\u001B[39;00m\n\u001B[0;32m   1907\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1908\u001B[0m X, y, groups \u001B[38;5;241m=\u001B[39m indexable(X, y, groups)\n\u001B[1;32m-> 1909\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m train, test \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter_indices(X, y, groups):\n\u001B[0;32m   1910\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m train, test\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\rozpoznawanie_dzwiekow_gitarowy\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2318\u001B[0m, in \u001B[0;36mStratifiedShuffleSplit._iter_indices\u001B[1;34m(self, X, y, groups)\u001B[0m\n\u001B[0;32m   2316\u001B[0m class_counts \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mbincount(y_indices)\n\u001B[0;32m   2317\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m np\u001B[38;5;241m.\u001B[39mmin(class_counts) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[1;32m-> 2318\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   2319\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe least populated class in y has only 1\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2320\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m member, which is too few. The minimum\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2321\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m number of groups for any class cannot\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2322\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m be less than 2.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2323\u001B[0m     )\n\u001B[0;32m   2325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_train \u001B[38;5;241m<\u001B[39m n_classes:\n\u001B[0;32m   2326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   2327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe train_size = \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m should be greater or \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2328\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mequal to the number of classes = \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (n_train, n_classes)\n\u001B[0;32m   2329\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
