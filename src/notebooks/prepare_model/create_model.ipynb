{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create Model",
   "id": "3deb9fee9efc9fb4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model setup",
   "id": "71c709bb2d6099b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T20:14:08.896003Z",
     "start_time": "2025-05-11T20:14:08.848621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import config\n",
    "\n",
    "data_path = config.SOUNDS_DATA_DIR_PATH\n",
    "splits_path=config.DATA_SPLIT_SAVE_DIR_PATH\n",
    "model_dir_path=config.MODEL_DIR_PATH\n",
    "\n",
    "n_fft = 2048 # Ile próbek bierze do okna na ktorym dokonuje transformaty\n",
    "hop_lenght = 1024 # O ile próbek przesuwa okno po każdej transformacie (Od tego zalezy wielkosc dataframe'a)\n",
    "sr = 22050 # Liczba próbek na sekunde (Od tego zalezy wielkosc dataframe'a)"
   ],
   "id": "9610e478be5cab9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bcom_\\Documents\\Projekty\\Rozpoznawanie_dzwiekow_gitarowych\\data\\prepared_data\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model creation process",
   "id": "9e44298ffbc67cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Create splits for models",
   "id": "d6c888191ab2cee8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T20:18:25.491815Z",
     "start_time": "2025-05-11T20:14:09.102126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.myscripts import func\n",
    "\n",
    "func.get_feature_combination_dataframe(['mel'], ['sound'], n_fft, hop_lenght, sr, data_path, True, 'sound_predict_df')\n",
    "func.get_feature_combination_dataframe(['mel'], ['string'], n_fft, hop_lenght, sr, data_path, True, 'string_predict_df')\n",
    "func.get_feature_combination_dataframe(['chroma', 'contrast'], ['sound_type'], n_fft, hop_lenght, sr, data_path, True, 'sound_type_predict_df')"
   ],
   "id": "dc5ce64280486213",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Training sound recognition model",
   "id": "fdf2ed2ef529a72f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T20:19:19.107106Z",
     "start_time": "2025-05-11T20:18:26.032701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.myscripts.train import ModelTrainer\n",
    "from src.myscripts.model import Conv1DClassifier\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from src.myscripts import prepare_data\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(f\"{config.DATAFRAMES_DIR_PATH}/sound_predict_df.csv\")\n",
    "prepare_data.prepare_data_for_model(data,-1, LabelEncoder, [0.7, 0.2, 0.1])\n",
    "\n",
    "x_train= np.load(os.path.join(splits_path, \"x_train.npy\"), allow_pickle=True)\n",
    "y_train= np.load(os.path.join(splits_path, \"y_train.npy\"), allow_pickle=True)\n",
    "x_val= np.load(os.path.join(splits_path, \"x_val.npy\"), allow_pickle=True)\n",
    "y_val= np.load(os.path.join(splits_path, \"y_val.npy\"), allow_pickle=True)\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_val = x_val.astype(np.float32)\n",
    "y_train = y_train.astype(\"long\")\n",
    "y_val = y_val.astype(\"long\")\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_shape = x_train.shape[1:]\n",
    "model = Conv1DClassifier(num_classes=43, input_shape=input_shape)\n",
    "\n",
    "model_trainer = ModelTrainer(model, device)\n",
    "model_trainer.train_model(train_dataset, val_dataset)\n",
    "# Ładujemy najlepszy model\n",
    "model = model_trainer.get_trained_model()\n",
    "\n",
    "torch.save(model.state_dict(),f\"{model_dir_path}/sound_recognition_model.pth\")"
   ],
   "id": "adbee4643a00c5b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared data saved in: C:\\Users\\Bcom_\\Documents\\Projekty\\Rozpoznawanie_dzwiekow_gitarowych\\data\\prepared_data.\n",
      "Data shape confirmation:\n",
      "x_train:(1015, 5632, 1)\n",
      "y_train:(1015,)\n",
      "x_val:(290, 5632, 1)\n",
      "y_val:(290,)\n",
      "x_test:(146, 5632, 1)\n",
      "y_test:(146,)\n",
      "\n",
      "Number of classes: 43\n",
      "Epoch 1/10, Loss: 4.0940, Accuracy: 0.1044\n",
      "Validation Loss: 2.8340, Validation Accuracy: 0.4517\n",
      "Epoch 2/10, Loss: 2.6495, Accuracy: 0.3576\n",
      "Validation Loss: 1.4175, Validation Accuracy: 0.8138\n",
      "Epoch 3/10, Loss: 1.8167, Accuracy: 0.5547\n",
      "Validation Loss: 0.7731, Validation Accuracy: 0.8724\n",
      "Epoch 4/10, Loss: 1.1693, Accuracy: 0.7025\n",
      "Validation Loss: 0.4122, Validation Accuracy: 0.9414\n",
      "Epoch 5/10, Loss: 0.8823, Accuracy: 0.7507\n",
      "Validation Loss: 0.2547, Validation Accuracy: 0.9586\n",
      "Epoch 6/10, Loss: 0.7062, Accuracy: 0.8227\n",
      "Validation Loss: 0.3004, Validation Accuracy: 0.9448\n",
      "Validation loss did not improve for 1 epochs.\n",
      "Epoch 7/10, Loss: 0.5701, Accuracy: 0.8473\n",
      "Validation Loss: 0.1357, Validation Accuracy: 0.9793\n",
      "Epoch 8/10, Loss: 0.4952, Accuracy: 0.8749\n",
      "Validation Loss: 0.1142, Validation Accuracy: 0.9828\n",
      "Epoch 9/10, Loss: 0.4278, Accuracy: 0.8966\n",
      "Validation Loss: 0.0478, Validation Accuracy: 0.9862\n",
      "Epoch 10/10, Loss: 0.2879, Accuracy: 0.9113\n",
      "Validation Loss: 0.0567, Validation Accuracy: 0.9897\n",
      "Validation loss did not improve for 1 epochs.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Training string recognition model",
   "id": "45f627ea30d77b61"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T20:19:49.495195Z",
     "start_time": "2025-05-11T20:19:19.224232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(f\"{config.DATAFRAMES_DIR_PATH}/string_predict_df.csv\")\n",
    "prepare_data.prepare_data_for_model(data,-1, LabelEncoder, [0.7, 0.2, 0.1])\n",
    "\n",
    "x_train= np.load(os.path.join(splits_path, \"x_train.npy\"), allow_pickle=True)\n",
    "y_train= np.load(os.path.join(splits_path, \"y_train.npy\"), allow_pickle=True)\n",
    "x_val= np.load(os.path.join(splits_path, \"x_val.npy\"), allow_pickle=True)\n",
    "y_val= np.load(os.path.join(splits_path, \"y_val.npy\"), allow_pickle=True)\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_val = x_val.astype(np.float32)\n",
    "y_train = y_train.astype(\"long\")\n",
    "y_val = y_val.astype(\"long\")\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_shape = x_train.shape[1:]\n",
    "model = Conv1DClassifier(num_classes=43, input_shape=input_shape)\n",
    "\n",
    "model_trainer = ModelTrainer(model, device)\n",
    "model_trainer.train_model(train_dataset, val_dataset)\n",
    "# Ładujemy najlepszy model\n",
    "model = model_trainer.get_trained_model()\n",
    "\n",
    "torch.save(model.state_dict(),f\"{model_dir_path}/string_recognition_model.pth\")"
   ],
   "id": "6e428e4b260107e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared data saved in: C:\\Users\\Bcom_\\Documents\\Projekty\\Rozpoznawanie_dzwiekow_gitarowych\\data\\prepared_data.\n",
      "Data shape confirmation:\n",
      "x_train:(1015, 5632, 1)\n",
      "y_train:(1015,)\n",
      "x_val:(290, 5632, 1)\n",
      "y_val:(290,)\n",
      "x_test:(146, 5632, 1)\n",
      "y_test:(146,)\n",
      "\n",
      "Number of classes: 2\n",
      "Epoch 1/10, Loss: 1.4103, Accuracy: 0.5350\n",
      "Validation Loss: 0.5042, Validation Accuracy: 0.8000\n",
      "Epoch 2/10, Loss: 0.5110, Accuracy: 0.7517\n",
      "Validation Loss: 0.2646, Validation Accuracy: 0.8621\n",
      "Epoch 3/10, Loss: 0.3017, Accuracy: 0.8788\n",
      "Validation Loss: 0.2358, Validation Accuracy: 0.9241\n",
      "Epoch 4/10, Loss: 0.2117, Accuracy: 0.9261\n",
      "Validation Loss: 0.1288, Validation Accuracy: 0.9379\n",
      "Epoch 5/10, Loss: 0.1270, Accuracy: 0.9586\n",
      "Validation Loss: 0.1146, Validation Accuracy: 0.9448\n",
      "Epoch 6/10, Loss: 0.1208, Accuracy: 0.9655\n",
      "Validation Loss: 0.0725, Validation Accuracy: 0.9621\n",
      "Epoch 7/10, Loss: 0.0760, Accuracy: 0.9675\n",
      "Validation Loss: 0.1025, Validation Accuracy: 0.9483\n",
      "Validation loss did not improve for 1 epochs.\n",
      "Epoch 8/10, Loss: 0.0687, Accuracy: 0.9724\n",
      "Validation Loss: 0.1114, Validation Accuracy: 0.9552\n",
      "Validation loss did not improve for 2 epochs.\n",
      "Epoch 9/10, Loss: 0.0705, Accuracy: 0.9803\n",
      "Validation Loss: 0.0574, Validation Accuracy: 0.9793\n",
      "Epoch 10/10, Loss: 0.0362, Accuracy: 0.9882\n",
      "Validation Loss: 0.0603, Validation Accuracy: 0.9793\n",
      "Validation loss did not improve for 1 epochs.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Training sound_type recognition model",
   "id": "7fcc7e3643af5782"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T20:19:55.412966Z",
     "start_time": "2025-05-11T20:19:49.573812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(f\"{config.DATAFRAMES_DIR_PATH}/sound_type_predict_df.csv\")\n",
    "prepare_data.prepare_data_for_model(data,-1, LabelEncoder, [0.7, 0.2, 0.1])\n",
    "\n",
    "x_train= np.load(os.path.join(splits_path, \"x_train.npy\"), allow_pickle=True)\n",
    "y_train= np.load(os.path.join(splits_path, \"y_train.npy\"), allow_pickle=True)\n",
    "x_val= np.load(os.path.join(splits_path, \"x_val.npy\"), allow_pickle=True)\n",
    "y_val= np.load(os.path.join(splits_path, \"y_val.npy\"), allow_pickle=True)\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_val = x_val.astype(np.float32)\n",
    "y_train = y_train.astype(\"long\")\n",
    "y_val = y_val.astype(\"long\")\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_shape = x_train.shape[1:]\n",
    "model = Conv1DClassifier(num_classes=43, input_shape=input_shape)\n",
    "\n",
    "model_trainer = ModelTrainer(model, device)\n",
    "model_trainer.train_model(train_dataset, val_dataset)\n",
    "# Ładujemy najlepszy model\n",
    "model = model_trainer.get_trained_model()\n",
    "\n",
    "torch.save(model.state_dict(),f\"{model_dir_path}/sound_type_recognition_model.pth\")"
   ],
   "id": "451a9e3f44d8b4c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared data saved in: C:\\Users\\Bcom_\\Documents\\Projekty\\Rozpoznawanie_dzwiekow_gitarowych\\data\\prepared_data.\n",
      "Data shape confirmation:\n",
      "x_train:(1015, 836, 1)\n",
      "y_train:(1015,)\n",
      "x_val:(290, 836, 1)\n",
      "y_val:(290,)\n",
      "x_test:(146, 836, 1)\n",
      "y_test:(146,)\n",
      "\n",
      "Number of classes: 3\n",
      "Epoch 1/10, Loss: 1.8366, Accuracy: 0.4227\n",
      "Validation Loss: 0.9560, Validation Accuracy: 0.5000\n",
      "Epoch 2/10, Loss: 1.0808, Accuracy: 0.5517\n",
      "Validation Loss: 0.6664, Validation Accuracy: 0.7690\n",
      "Epoch 3/10, Loss: 0.7149, Accuracy: 0.7005\n",
      "Validation Loss: 0.5252, Validation Accuracy: 0.8000\n",
      "Epoch 4/10, Loss: 0.5791, Accuracy: 0.7714\n",
      "Validation Loss: 0.5060, Validation Accuracy: 0.8034\n",
      "Epoch 5/10, Loss: 0.5151, Accuracy: 0.7911\n",
      "Validation Loss: 0.4399, Validation Accuracy: 0.8138\n",
      "Epoch 6/10, Loss: 0.5182, Accuracy: 0.7951\n",
      "Validation Loss: 0.4314, Validation Accuracy: 0.8138\n",
      "Epoch 7/10, Loss: 0.4814, Accuracy: 0.8108\n",
      "Validation Loss: 0.4268, Validation Accuracy: 0.8241\n",
      "Epoch 8/10, Loss: 0.4597, Accuracy: 0.8197\n",
      "Validation Loss: 0.4280, Validation Accuracy: 0.8103\n",
      "Validation loss did not improve for 1 epochs.\n",
      "Epoch 9/10, Loss: 0.4567, Accuracy: 0.8167\n",
      "Validation Loss: 0.3817, Validation Accuracy: 0.8345\n",
      "Epoch 10/10, Loss: 0.4668, Accuracy: 0.8167\n",
      "Validation Loss: 0.3971, Validation Accuracy: 0.8241\n",
      "Validation loss did not improve for 1 epochs.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "TU ZAPISUJE STATE 3 MODELI MUSZE WYKOMBINOWAC CZY DA SIE ZROBIC SKRYPT PYTHONA KTORY DLA PODANEJ PROBKI DZWIEKOWEJ BEDZIE PRZEWIDYWAL LABEL DLA KAZDEGO Z TYCH MODELI A NASTEPNIE LACZYL TE PRZEWIDZIANE LABELE I PODAWAL GO JAKO WYNIK.\n",
    "CHYBA BEDE MUSIAL WZIAC POD UWAGE TO ZE PRZEWIDYWANY LABEL JEST PO LABELENCODER I MUSZE POMYSLEC JAK PRZYWROCIC ZEBY PRZEWIDZIANA ETYKIETE ZAMIENIAL NA FAKTYCZNY ZROZUMIALY LABEL"
   ],
   "id": "be769604c126358a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
