{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create Model",
   "id": "3deb9fee9efc9fb4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T19:13:58.907096Z",
     "start_time": "2025-05-14T19:13:48.639738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.myscripts.train import ModelTrainer\n",
    "from src.myscripts.model import Conv1DClassifier\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from src.myscripts import prepare_data\n",
    "import pandas as pd\n",
    "import config"
   ],
   "id": "2648ac015e9f9374",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bcom_\\Documents\\Projekty\\Rozpoznawanie_dzwiekow_gitarowych\\data\\prepared_data\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model setup",
   "id": "71c709bb2d6099b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T19:13:59.558832Z",
     "start_time": "2025-05-14T19:13:59.547786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "data_path = config.SOUNDS_DATA_DIR_PATH\n",
    "splits_path=config.DATA_SPLIT_SAVE_DIR_PATH\n",
    "model_dir_path=config.MODEL_DIR_PATH\n",
    "\n",
    "n_fft = 2048 # Ile próbek bierze do okna na ktorym dokonuje transformaty\n",
    "hop_lenght = 1024 # O ile próbek przesuwa okno po każdej transformacie (Od tego zalezy wielkosc dataframe'a)\n",
    "sr = 22050 # Liczba próbek na sekunde (Od tego zalezy wielkosc dataframe'a)"
   ],
   "id": "9610e478be5cab9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model creation process",
   "id": "9e44298ffbc67cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Create splits for models",
   "id": "d6c888191ab2cee8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T19:16:49.117443Z",
     "start_time": "2025-05-14T19:13:59.574262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.myscripts import func\n",
    "\n",
    "func.get_feature_combination_dataframe(['mel'], ['sound'], n_fft, hop_lenght, sr, data_path, True, 'sound_predict_df')\n",
    "func.get_feature_combination_dataframe(['mel'], ['string'], n_fft, hop_lenght, sr, data_path, True, 'string_predict_df')\n",
    "func.get_feature_combination_dataframe(['chroma', 'contrast'], ['sound_type'], n_fft, hop_lenght, sr, data_path, True, 'sound_type_predict_df')"
   ],
   "id": "dc5ce64280486213",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Po splaszczeniu mel: (1451, 5632)\n",
      "Po splaszczeniu mel: (1451, 5632)\n",
      "Po splaszczeniu mel: (1451, 5632)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Training sound recognition model",
   "id": "fdf2ed2ef529a72f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T19:05:17.661763Z",
     "start_time": "2025-05-14T19:05:04.979851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "data = pd.read_csv(f\"{config.DATAFRAMES_DIR_PATH}/sound_predict_df.csv\")\n",
    "prepare_data.prepare_data_for_model(data,-1, LabelEncoder, [0.7, 0.2, 0.1], encoder_file_name=\"sound_encoder.joblib\")\n",
    "\n",
    "x_train= np.load(os.path.join(splits_path, \"x_train.npy\"), allow_pickle=True)\n",
    "y_train= np.load(os.path.join(splits_path, \"y_train.npy\"), allow_pickle=True)\n",
    "x_val= np.load(os.path.join(splits_path, \"x_val.npy\"), allow_pickle=True)\n",
    "y_val= np.load(os.path.join(splits_path, \"y_val.npy\"), allow_pickle=True)\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_val = x_val.astype(np.float32)\n",
    "y_train = y_train.astype(\"long\")\n",
    "y_val = y_val.astype(\"long\")\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_shape = x_train.shape[1:]\n",
    "model = Conv1DClassifier(num_classes=43, input_shape=input_shape)\n",
    "\n",
    "model_trainer = ModelTrainer(model, device)\n",
    "model_trainer.train_model(train_dataset, val_dataset, epochs=20)\n",
    "# Ładujemy najlepszy model\n",
    "model = model_trainer.get_trained_model()\n",
    "\n",
    "torch.save(model.state_dict(),f\"{model_dir_path}/sound_recognition_model.pth\")"
   ],
   "id": "adbee4643a00c5b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared data saved in: C:\\Users\\Bcom_\\Documents\\Projekty\\Rozpoznawanie_dzwiekow_gitarowych\\data\\prepared_data.\n",
      "Data shape confirmation:\n",
      "x_train:(1015, 5632, 1)\n",
      "y_train:(1015,)\n",
      "x_val:(290, 5632, 1)\n",
      "y_val:(290,)\n",
      "x_test:(146, 5632, 1)\n",
      "y_test:(146,)\n",
      "\n",
      "Number of classes: 43\n",
      "x_train shape: (1015, 5632, 1)\n",
      "Epoch 1/20, Loss: 3.8805, Accuracy: 0.1044\n",
      "Validation Loss: 2.7634, Validation Accuracy: 0.4310\n",
      "Epoch 2/20, Loss: 2.7868, Accuracy: 0.3517\n",
      "Validation Loss: 1.4649, Validation Accuracy: 0.7172\n",
      "Epoch 3/20, Loss: 1.7426, Accuracy: 0.5596\n",
      "Validation Loss: 0.7459, Validation Accuracy: 0.8724\n",
      "Epoch 4/20, Loss: 1.2262, Accuracy: 0.6798\n",
      "Validation Loss: 0.4514, Validation Accuracy: 0.9207\n",
      "Epoch 5/20, Loss: 0.9017, Accuracy: 0.7596\n",
      "Validation Loss: 0.2873, Validation Accuracy: 0.9483\n",
      "Epoch 6/20, Loss: 0.6772, Accuracy: 0.8108\n",
      "Validation Loss: 0.2091, Validation Accuracy: 0.9552\n",
      "Epoch 7/20, Loss: 0.6232, Accuracy: 0.8552\n",
      "Validation Loss: 0.1320, Validation Accuracy: 0.9759\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[19]\u001B[39m\u001B[32m, line 28\u001B[39m\n\u001B[32m     25\u001B[39m model = Conv1DClassifier(num_classes=\u001B[32m43\u001B[39m, input_shape=input_shape)\n\u001B[32m     27\u001B[39m model_trainer = ModelTrainer(model, device)\n\u001B[32m---> \u001B[39m\u001B[32m28\u001B[39m \u001B[43mmodel_trainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m20\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     29\u001B[39m \u001B[38;5;66;03m# Ładujemy najlepszy model\u001B[39;00m\n\u001B[32m     30\u001B[39m model = model_trainer.get_trained_model()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Projekty\\Rozpoznawanie_dzwiekow_gitarowych\\src\\myscripts\\train.py:42\u001B[39m, in \u001B[36mModelTrainer.train_model\u001B[39m\u001B[34m(self, train_dataset, val_dataset, epochs, early_stopping_rounds)\u001B[39m\n\u001B[32m     39\u001B[39m loss.backward()\n\u001B[32m     40\u001B[39m \u001B[38;5;28mself\u001B[39m.optimizer.step()\n\u001B[32m---> \u001B[39m\u001B[32m42\u001B[39m running_loss += \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     43\u001B[39m _, preds = torch.max(outputs, \u001B[32m1\u001B[39m)\n\u001B[32m     44\u001B[39m correct += torch.sum(preds == labels)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Training string recognition model",
   "id": "45f627ea30d77b61"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:38:41.576537800Z",
     "start_time": "2025-05-14T18:18:26.569601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "data = pd.read_csv(f\"{config.DATAFRAMES_DIR_PATH}/string_predict_df.csv\")\n",
    "prepare_data.prepare_data_for_model(data,-1, LabelEncoder, [0.7, 0.2, 0.1], encoder_file_name=\"string_encoder.joblib\")\n",
    "\n",
    "x_train= np.load(os.path.join(splits_path, \"x_train.npy\"), allow_pickle=True)\n",
    "y_train= np.load(os.path.join(splits_path, \"y_train.npy\"), allow_pickle=True)\n",
    "x_val= np.load(os.path.join(splits_path, \"x_val.npy\"), allow_pickle=True)\n",
    "y_val= np.load(os.path.join(splits_path, \"y_val.npy\"), allow_pickle=True)\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_val = x_val.astype(np.float32)\n",
    "y_train = y_train.astype(\"long\")\n",
    "y_val = y_val.astype(\"long\")\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_shape = x_train.shape[1:]\n",
    "model = Conv1DClassifier(num_classes=2, input_shape=input_shape)\n",
    "\n",
    "model_trainer = ModelTrainer(model, device)\n",
    "model_trainer.train_model(train_dataset, val_dataset, epochs=20)\n",
    "# Ładujemy najlepszy model\n",
    "model = model_trainer.get_trained_model()\n",
    "\n",
    "torch.save(model.state_dict(),f\"{model_dir_path}/string_recognition_model.pth\")"
   ],
   "id": "6e428e4b260107e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared data saved in: C:\\Users\\Bcom_\\Documents\\Projekty\\Rozpoznawanie_dzwiekow_gitarowych\\data\\prepared_data.\n",
      "Data shape confirmation:\n",
      "x_train:(1015, 5632, 1)\n",
      "y_train:(1015,)\n",
      "x_val:(290, 5632, 1)\n",
      "y_val:(290,)\n",
      "x_test:(146, 5632, 1)\n",
      "y_test:(146,)\n",
      "\n",
      "Number of classes: 2\n",
      "Epoch 1/20, Loss: 0.8176, Accuracy: 0.6680\n",
      "Validation Loss: 0.3724, Validation Accuracy: 0.8103\n",
      "Epoch 2/20, Loss: 0.3234, Accuracy: 0.8956\n",
      "Validation Loss: 0.1607, Validation Accuracy: 0.9448\n",
      "Epoch 3/20, Loss: 0.1557, Accuracy: 0.9330\n",
      "Validation Loss: 0.1072, Validation Accuracy: 0.9414\n",
      "Epoch 4/20, Loss: 0.1293, Accuracy: 0.9665\n",
      "Validation Loss: 0.0650, Validation Accuracy: 0.9828\n",
      "Epoch 5/20, Loss: 0.1126, Accuracy: 0.9695\n",
      "Validation Loss: 0.0569, Validation Accuracy: 0.9759\n",
      "Epoch 6/20, Loss: 0.0689, Accuracy: 0.9773\n",
      "Validation Loss: 0.0581, Validation Accuracy: 0.9828\n",
      "Validation loss did not improve for 1 epochs.\n",
      "Epoch 7/20, Loss: 0.0758, Accuracy: 0.9793\n",
      "Validation Loss: 0.0677, Validation Accuracy: 0.9655\n",
      "Validation loss did not improve for 2 epochs.\n",
      "Epoch 8/20, Loss: 0.0381, Accuracy: 0.9852\n",
      "Validation Loss: 0.0519, Validation Accuracy: 0.9862\n",
      "Epoch 9/20, Loss: 0.0460, Accuracy: 0.9892\n",
      "Validation Loss: 0.0889, Validation Accuracy: 0.9724\n",
      "Validation loss did not improve for 1 epochs.\n",
      "Epoch 10/20, Loss: 0.0332, Accuracy: 0.9882\n",
      "Validation Loss: 0.0639, Validation Accuracy: 0.9793\n",
      "Validation loss did not improve for 2 epochs.\n",
      "Epoch 11/20, Loss: 0.0409, Accuracy: 0.9892\n",
      "Validation Loss: 0.0476, Validation Accuracy: 0.9897\n",
      "Epoch 12/20, Loss: 0.0260, Accuracy: 0.9892\n",
      "Validation Loss: 0.0785, Validation Accuracy: 0.9690\n",
      "Validation loss did not improve for 1 epochs.\n",
      "Epoch 13/20, Loss: 0.0384, Accuracy: 0.9872\n",
      "Validation Loss: 0.0343, Validation Accuracy: 0.9897\n",
      "Epoch 14/20, Loss: 0.0154, Accuracy: 0.9961\n",
      "Validation Loss: 0.0462, Validation Accuracy: 0.9897\n",
      "Validation loss did not improve for 1 epochs.\n",
      "Epoch 15/20, Loss: 0.0187, Accuracy: 0.9970\n",
      "Validation Loss: 0.0259, Validation Accuracy: 0.9862\n",
      "Epoch 16/20, Loss: 0.0179, Accuracy: 0.9931\n",
      "Validation Loss: 0.1084, Validation Accuracy: 0.9655\n",
      "Validation loss did not improve for 1 epochs.\n",
      "Epoch 17/20, Loss: 0.0331, Accuracy: 0.9921\n",
      "Validation Loss: 0.0131, Validation Accuracy: 0.9931\n",
      "Epoch 18/20, Loss: 0.0136, Accuracy: 0.9961\n",
      "Validation Loss: 0.0464, Validation Accuracy: 0.9897\n",
      "Validation loss did not improve for 1 epochs.\n",
      "Epoch 19/20, Loss: 0.0057, Accuracy: 0.9980\n",
      "Validation Loss: 0.0277, Validation Accuracy: 0.9897\n",
      "Validation loss did not improve for 2 epochs.\n",
      "Epoch 20/20, Loss: 0.0027, Accuracy: 1.0000\n",
      "Validation Loss: 0.0231, Validation Accuracy: 0.9931\n",
      "Validation loss did not improve for 3 epochs.\n",
      "Early stopping triggered after 3 epochs without improvement.\n",
      "Early stopping triggered. Stopping training.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Training sound_type recognition model",
   "id": "7fcc7e3643af5782"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T18:38:41.577560900Z",
     "start_time": "2025-05-14T18:19:19.662500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(f\"{config.DATAFRAMES_DIR_PATH}/sound_type_predict_df.csv\")\n",
    "prepare_data.prepare_data_for_model(data,-1, LabelEncoder, [0.7, 0.2, 0.1], encoder_file_name=\"sound_type_encoder.joblib\")\n",
    "\n",
    "x_train= np.load(os.path.join(splits_path, \"x_train.npy\"), allow_pickle=True)\n",
    "y_train= np.load(os.path.join(splits_path, \"y_train.npy\"), allow_pickle=True)\n",
    "x_val= np.load(os.path.join(splits_path, \"x_val.npy\"), allow_pickle=True)\n",
    "y_val= np.load(os.path.join(splits_path, \"y_val.npy\"), allow_pickle=True)\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_val = x_val.astype(np.float32)\n",
    "y_train = y_train.astype(\"long\")\n",
    "y_val = y_val.astype(\"long\")\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_shape = x_train.shape[1:]\n",
    "model = Conv1DClassifier(num_classes=3, input_shape=input_shape)\n",
    "\n",
    "model_trainer = ModelTrainer(model, device)\n",
    "model_trainer.train_model(train_dataset, val_dataset, epochs=20)\n",
    "# Ładujemy najlepszy model\n",
    "model = model_trainer.get_trained_model()\n",
    "\n",
    "torch.save(model.state_dict(),f\"{model_dir_path}/sound_type_recognition_model.pth\")"
   ],
   "id": "451a9e3f44d8b4c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared data saved in: C:\\Users\\Bcom_\\Documents\\Projekty\\Rozpoznawanie_dzwiekow_gitarowych\\data\\prepared_data.\n",
      "Data shape confirmation:\n",
      "x_train:(1015, 836, 1)\n",
      "y_train:(1015,)\n",
      "x_val:(290, 836, 1)\n",
      "y_val:(290,)\n",
      "x_test:(146, 836, 1)\n",
      "y_test:(146,)\n",
      "\n",
      "Number of classes: 3\n",
      "Epoch 1/20, Loss: 1.2488, Accuracy: 0.4818\n",
      "Validation Loss: 0.9279, Validation Accuracy: 0.7862\n",
      "Epoch 2/20, Loss: 0.7619, Accuracy: 0.7143\n",
      "Validation Loss: 0.5076, Validation Accuracy: 0.7931\n",
      "Epoch 3/20, Loss: 0.5329, Accuracy: 0.7783\n",
      "Validation Loss: 0.4467, Validation Accuracy: 0.8138\n",
      "Epoch 4/20, Loss: 0.4708, Accuracy: 0.8039\n",
      "Validation Loss: 0.4055, Validation Accuracy: 0.8276\n",
      "Epoch 5/20, Loss: 0.4601, Accuracy: 0.8197\n",
      "Validation Loss: 0.3906, Validation Accuracy: 0.8345\n",
      "Epoch 6/20, Loss: 0.4528, Accuracy: 0.8197\n",
      "Validation Loss: 0.4073, Validation Accuracy: 0.8207\n",
      "Validation loss did not improve for 1 epochs.\n",
      "Epoch 7/20, Loss: 0.4351, Accuracy: 0.8187\n",
      "Validation Loss: 0.3795, Validation Accuracy: 0.8345\n",
      "Epoch 8/20, Loss: 0.4314, Accuracy: 0.8167\n",
      "Validation Loss: 0.3869, Validation Accuracy: 0.8345\n",
      "Validation loss did not improve for 1 epochs.\n",
      "Epoch 9/20, Loss: 0.4215, Accuracy: 0.8246\n",
      "Validation Loss: 0.3803, Validation Accuracy: 0.8345\n",
      "Validation loss did not improve for 2 epochs.\n",
      "Epoch 10/20, Loss: 0.4064, Accuracy: 0.8246\n",
      "Validation Loss: 0.4600, Validation Accuracy: 0.8069\n",
      "Validation loss did not improve for 3 epochs.\n",
      "Early stopping triggered after 3 epochs without improvement.\n",
      "Early stopping triggered. Stopping training.\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
