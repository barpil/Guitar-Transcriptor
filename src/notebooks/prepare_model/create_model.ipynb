{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create Model",
   "id": "3deb9fee9efc9fb4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model setup",
   "id": "71c709bb2d6099b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:18:22.695078Z",
     "start_time": "2025-05-12T22:18:22.655203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import config\n",
    "\n",
    "data_path = config.SOUNDS_DATA_DIR_PATH\n",
    "splits_path=config.DATA_SPLIT_SAVE_DIR_PATH\n",
    "model_dir_path=config.MODEL_DIR_PATH\n",
    "\n",
    "n_fft = 2048 # Ile próbek bierze do okna na ktorym dokonuje transformaty\n",
    "hop_lenght = 1024 # O ile próbek przesuwa okno po każdej transformacie (Od tego zalezy wielkosc dataframe'a)\n",
    "sr = 22050 # Liczba próbek na sekunde (Od tego zalezy wielkosc dataframe'a)"
   ],
   "id": "9610e478be5cab9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bcom_\\Documents\\Projekty\\Rozpoznawanie_dzwiekow_gitarowych\\data\\prepared_data\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model creation process",
   "id": "9e44298ffbc67cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Create splits for models",
   "id": "d6c888191ab2cee8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:22:34.323559Z",
     "start_time": "2025-05-12T22:18:22.762011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.myscripts import func\n",
    "\n",
    "func.get_feature_combination_dataframe(['mel'], ['sound'], n_fft, hop_lenght, sr, data_path, True, 'sound_predict_df')\n",
    "func.get_feature_combination_dataframe(['mel'], ['string'], n_fft, hop_lenght, sr, data_path, True, 'string_predict_df')\n",
    "func.get_feature_combination_dataframe(['chroma', 'contrast'], ['sound_type'], n_fft, hop_lenght, sr, data_path, True, 'sound_type_predict_df')"
   ],
   "id": "dc5ce64280486213",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Training sound recognition model",
   "id": "fdf2ed2ef529a72f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:23:24.451902Z",
     "start_time": "2025-05-12T22:22:34.852443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.myscripts.train import ModelTrainer\n",
    "from src.myscripts.model import Conv1DClassifier\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from src.myscripts import prepare_data\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(f\"{config.DATAFRAMES_DIR_PATH}/sound_predict_df.csv\")\n",
    "prepare_data.prepare_data_for_model(data,-1, LabelEncoder, [0.7, 0.2, 0.1], encoder_file_name=\"sound_encoder.joblib\")\n",
    "\n",
    "x_train= np.load(os.path.join(splits_path, \"x_train.npy\"), allow_pickle=True)\n",
    "y_train= np.load(os.path.join(splits_path, \"y_train.npy\"), allow_pickle=True)\n",
    "x_val= np.load(os.path.join(splits_path, \"x_val.npy\"), allow_pickle=True)\n",
    "y_val= np.load(os.path.join(splits_path, \"y_val.npy\"), allow_pickle=True)\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_val = x_val.astype(np.float32)\n",
    "y_train = y_train.astype(\"long\")\n",
    "y_val = y_val.astype(\"long\")\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_shape = x_train.shape[1:]\n",
    "model = Conv1DClassifier(num_classes=43, input_shape=input_shape)\n",
    "\n",
    "model_trainer = ModelTrainer(model, device)\n",
    "model_trainer.train_model(train_dataset, val_dataset)\n",
    "# Ładujemy najlepszy model\n",
    "model = model_trainer.get_trained_model()\n",
    "\n",
    "torch.save(model.state_dict(),f\"{model_dir_path}/sound_recognition_model.pth\")"
   ],
   "id": "adbee4643a00c5b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared data saved in: C:\\Users\\Bcom_\\Documents\\Projekty\\Rozpoznawanie_dzwiekow_gitarowych\\data\\prepared_data.\n",
      "Data shape confirmation:\n",
      "x_train:(1015, 5632, 1)\n",
      "y_train:(1015,)\n",
      "x_val:(290, 5632, 1)\n",
      "y_val:(290,)\n",
      "x_test:(146, 5632, 1)\n",
      "y_test:(146,)\n",
      "\n",
      "Number of classes: 43\n",
      "Epoch 1/10, Loss: 4.0975, Accuracy: 0.1123\n",
      "Validation Loss: 2.6459, Validation Accuracy: 0.4862\n",
      "Epoch 2/10, Loss: 2.5769, Accuracy: 0.3645\n",
      "Validation Loss: 1.2723, Validation Accuracy: 0.7862\n",
      "Epoch 3/10, Loss: 1.7485, Accuracy: 0.5626\n",
      "Validation Loss: 0.7022, Validation Accuracy: 0.8621\n",
      "Epoch 4/10, Loss: 1.1805, Accuracy: 0.6985\n",
      "Validation Loss: 0.3652, Validation Accuracy: 0.9276\n",
      "Epoch 5/10, Loss: 0.9490, Accuracy: 0.7517\n",
      "Validation Loss: 0.3492, Validation Accuracy: 0.9586\n",
      "Epoch 6/10, Loss: 0.7793, Accuracy: 0.8177\n",
      "Validation Loss: 0.2395, Validation Accuracy: 0.9655\n",
      "Epoch 7/10, Loss: 0.6030, Accuracy: 0.8483\n",
      "Validation Loss: 0.1324, Validation Accuracy: 0.9793\n",
      "Epoch 8/10, Loss: 0.4455, Accuracy: 0.8887\n",
      "Validation Loss: 0.1077, Validation Accuracy: 0.9862\n",
      "Epoch 9/10, Loss: 0.3728, Accuracy: 0.9034\n",
      "Validation Loss: 0.0885, Validation Accuracy: 0.9793\n",
      "Epoch 10/10, Loss: 0.3279, Accuracy: 0.9054\n",
      "Validation Loss: 0.0725, Validation Accuracy: 0.9793\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Training string recognition model",
   "id": "45f627ea30d77b61"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:24:08.715987Z",
     "start_time": "2025-05-12T22:23:24.578544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(f\"{config.DATAFRAMES_DIR_PATH}/string_predict_df.csv\")\n",
    "prepare_data.prepare_data_for_model(data,-1, LabelEncoder, [0.7, 0.2, 0.1], encoder_file_name=\"string_encoder.joblib\")\n",
    "\n",
    "x_train= np.load(os.path.join(splits_path, \"x_train.npy\"), allow_pickle=True)\n",
    "y_train= np.load(os.path.join(splits_path, \"y_train.npy\"), allow_pickle=True)\n",
    "x_val= np.load(os.path.join(splits_path, \"x_val.npy\"), allow_pickle=True)\n",
    "y_val= np.load(os.path.join(splits_path, \"y_val.npy\"), allow_pickle=True)\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_val = x_val.astype(np.float32)\n",
    "y_train = y_train.astype(\"long\")\n",
    "y_val = y_val.astype(\"long\")\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_shape = x_train.shape[1:]\n",
    "model = Conv1DClassifier(num_classes=43, input_shape=input_shape)\n",
    "\n",
    "model_trainer = ModelTrainer(model, device)\n",
    "model_trainer.train_model(train_dataset, val_dataset, epochs=20)\n",
    "# Ładujemy najlepszy model\n",
    "model = model_trainer.get_trained_model()\n",
    "\n",
    "torch.save(model.state_dict(),f\"{model_dir_path}/string_recognition_model.pth\")"
   ],
   "id": "6e428e4b260107e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared data saved in: C:\\Users\\Bcom_\\Documents\\Projekty\\Rozpoznawanie_dzwiekow_gitarowych\\data\\prepared_data.\n",
      "Data shape confirmation:\n",
      "x_train:(1015, 5632, 1)\n",
      "y_train:(1015,)\n",
      "x_val:(290, 5632, 1)\n",
      "y_val:(290,)\n",
      "x_test:(146, 5632, 1)\n",
      "y_test:(146,)\n",
      "\n",
      "Number of classes: 2\n",
      "True\n",
      "Epoch 1/20, Loss: 1.4205, Accuracy: 0.5271\n",
      "Validation Loss: 0.4511, Validation Accuracy: 0.8345\n",
      "Epoch 2/20, Loss: 0.5106, Accuracy: 0.7931\n",
      "Validation Loss: 0.2530, Validation Accuracy: 0.8966\n",
      "Epoch 3/20, Loss: 0.2993, Accuracy: 0.8877\n",
      "Validation Loss: 0.1493, Validation Accuracy: 0.9310\n",
      "Epoch 4/20, Loss: 0.1656, Accuracy: 0.9409\n",
      "Validation Loss: 0.0828, Validation Accuracy: 0.9586\n",
      "Epoch 5/20, Loss: 0.1276, Accuracy: 0.9547\n",
      "Validation Loss: 0.0556, Validation Accuracy: 0.9862\n",
      "Epoch 6/20, Loss: 0.0929, Accuracy: 0.9655\n",
      "Validation Loss: 0.0596, Validation Accuracy: 0.9724\n",
      "Validation loss did not improve for 1 epochs.\n",
      "Epoch 7/20, Loss: 0.0585, Accuracy: 0.9764\n",
      "Validation Loss: 0.0375, Validation Accuracy: 0.9862\n",
      "Epoch 8/20, Loss: 0.0514, Accuracy: 0.9803\n",
      "Validation Loss: 0.0462, Validation Accuracy: 0.9759\n",
      "Validation loss did not improve for 1 epochs.\n",
      "Epoch 9/20, Loss: 0.0752, Accuracy: 0.9803\n",
      "Validation Loss: 0.0227, Validation Accuracy: 1.0000\n",
      "Epoch 10/20, Loss: 0.0422, Accuracy: 0.9892\n",
      "Validation Loss: 0.0222, Validation Accuracy: 0.9966\n",
      "Epoch 11/20, Loss: 0.0285, Accuracy: 0.9911\n",
      "Validation Loss: 0.0342, Validation Accuracy: 0.9862\n",
      "Validation loss did not improve for 1 epochs.\n",
      "Epoch 12/20, Loss: 0.0421, Accuracy: 0.9901\n",
      "Validation Loss: 0.0393, Validation Accuracy: 0.9828\n",
      "Validation loss did not improve for 2 epochs.\n",
      "Epoch 13/20, Loss: 0.0397, Accuracy: 0.9862\n",
      "Validation Loss: 0.0207, Validation Accuracy: 0.9897\n",
      "Epoch 14/20, Loss: 0.0153, Accuracy: 0.9970\n",
      "Validation Loss: 0.0269, Validation Accuracy: 0.9828\n",
      "Validation loss did not improve for 1 epochs.\n",
      "Epoch 15/20, Loss: 0.0140, Accuracy: 0.9951\n",
      "Validation Loss: 0.0425, Validation Accuracy: 0.9897\n",
      "Validation loss did not improve for 2 epochs.\n",
      "Epoch 16/20, Loss: 0.0421, Accuracy: 0.9872\n",
      "Validation Loss: 0.0480, Validation Accuracy: 0.9862\n",
      "Validation loss did not improve for 3 epochs.\n",
      "Early stopping triggered after 3 epochs without improvement.\n",
      "Early stopping triggered. Stopping training.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Training sound_type recognition model",
   "id": "7fcc7e3643af5782"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:24:16.892946Z",
     "start_time": "2025-05-12T22:24:08.792794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(f\"{config.DATAFRAMES_DIR_PATH}/sound_type_predict_df.csv\")\n",
    "prepare_data.prepare_data_for_model(data,-1, LabelEncoder, [0.7, 0.2, 0.1], encoder_file_name=\"sound_type_encoder.joblib\")\n",
    "\n",
    "x_train= np.load(os.path.join(splits_path, \"x_train.npy\"), allow_pickle=True)\n",
    "y_train= np.load(os.path.join(splits_path, \"y_train.npy\"), allow_pickle=True)\n",
    "x_val= np.load(os.path.join(splits_path, \"x_val.npy\"), allow_pickle=True)\n",
    "y_val= np.load(os.path.join(splits_path, \"y_val.npy\"), allow_pickle=True)\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_val = x_val.astype(np.float32)\n",
    "y_train = y_train.astype(\"long\")\n",
    "y_val = y_val.astype(\"long\")\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_shape = x_train.shape[1:]\n",
    "model = Conv1DClassifier(num_classes=43, input_shape=input_shape)\n",
    "\n",
    "model_trainer = ModelTrainer(model, device)\n",
    "model_trainer.train_model(train_dataset, val_dataset, epochs=20)\n",
    "# Ładujemy najlepszy model\n",
    "model = model_trainer.get_trained_model()\n",
    "\n",
    "torch.save(model.state_dict(),f\"{model_dir_path}/sound_type_recognition_model.pth\")"
   ],
   "id": "451a9e3f44d8b4c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared data saved in: C:\\Users\\Bcom_\\Documents\\Projekty\\Rozpoznawanie_dzwiekow_gitarowych\\data\\prepared_data.\n",
      "Data shape confirmation:\n",
      "x_train:(1015, 836, 1)\n",
      "y_train:(1015,)\n",
      "x_val:(290, 836, 1)\n",
      "y_val:(290,)\n",
      "x_test:(146, 836, 1)\n",
      "y_test:(146,)\n",
      "\n",
      "Number of classes: 3\n",
      "Epoch 1/20, Loss: 1.9498, Accuracy: 0.3901\n",
      "Validation Loss: 0.9899, Validation Accuracy: 0.5000\n",
      "Epoch 2/20, Loss: 1.0961, Accuracy: 0.5429\n",
      "Validation Loss: 0.6635, Validation Accuracy: 0.8000\n",
      "Epoch 3/20, Loss: 0.6932, Accuracy: 0.7241\n",
      "Validation Loss: 0.4832, Validation Accuracy: 0.8000\n",
      "Epoch 4/20, Loss: 0.5615, Accuracy: 0.7695\n",
      "Validation Loss: 0.4325, Validation Accuracy: 0.8138\n",
      "Epoch 5/20, Loss: 0.5240, Accuracy: 0.7833\n",
      "Validation Loss: 0.4081, Validation Accuracy: 0.8241\n",
      "Epoch 6/20, Loss: 0.4785, Accuracy: 0.7931\n",
      "Validation Loss: 0.3901, Validation Accuracy: 0.8310\n",
      "Epoch 7/20, Loss: 0.4829, Accuracy: 0.8010\n",
      "Validation Loss: 0.3931, Validation Accuracy: 0.8310\n",
      "Validation loss did not improve for 1 epochs.\n",
      "Epoch 8/20, Loss: 0.4512, Accuracy: 0.8207\n",
      "Validation Loss: 0.3804, Validation Accuracy: 0.8345\n",
      "Epoch 9/20, Loss: 0.4415, Accuracy: 0.8059\n",
      "Validation Loss: 0.4124, Validation Accuracy: 0.8172\n",
      "Validation loss did not improve for 1 epochs.\n",
      "Epoch 10/20, Loss: 0.4394, Accuracy: 0.8187\n",
      "Validation Loss: 0.3729, Validation Accuracy: 0.8345\n",
      "Epoch 11/20, Loss: 0.4347, Accuracy: 0.8227\n",
      "Validation Loss: 0.3754, Validation Accuracy: 0.8345\n",
      "Validation loss did not improve for 1 epochs.\n",
      "Epoch 12/20, Loss: 0.4189, Accuracy: 0.8266\n",
      "Validation Loss: 0.3834, Validation Accuracy: 0.8310\n",
      "Validation loss did not improve for 2 epochs.\n",
      "Epoch 13/20, Loss: 0.4075, Accuracy: 0.8256\n",
      "Validation Loss: 0.3902, Validation Accuracy: 0.8310\n",
      "Validation loss did not improve for 3 epochs.\n",
      "Early stopping triggered after 3 epochs without improvement.\n",
      "Early stopping triggered. Stopping training.\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
